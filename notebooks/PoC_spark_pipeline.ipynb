{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4dd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/birgitte/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Union \n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, udf, monotonically_increasing_id\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, IntegerType, StructType, StructField\n",
    "\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, StopWordsRemover, Tokenizer, SQLTransformer, VectorAssembler\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.linalg import Vector, Vectors\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.download('wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc004e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(Transformer, HasInputCol, HasOutputCol):\n",
    "    \"\"\"\n",
    "    A custom transformer that lemmatizes a text column using WordNetLemmatizer from the nltk package.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputCol=\"filtered_tokens\", outputCol=\"lemmatized\"):\n",
    "        super(Lemmatizer, self).__init__()\n",
    "        self.inputCol = Param(self, \"inputCol\", \"input column name\")\n",
    "        self.outputCol = Param(self, \"outputCol\", \"output column name\")\n",
    "        self._setDefault(inputCol=inputCol, outputCol=outputCol)\n",
    "\n",
    "    def getLemma(self, tokens):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def _transform(self, df):\n",
    "        lemma_udf = udf(lambda tokens: self.getLemma(tokens), ArrayType(StringType()))\n",
    "        return df.withColumn(self.getOutputCol(), lemma_udf(df[self.getInputCol()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c95bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 21:46:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder.appName(\"Spark ETL and LDA model pipeline\")\n",
    "    .master(\"local[4]\")                \n",
    "    .config(\"spark.driver.memory\", \"8g\") \n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\")\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4417a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(path: str) -> pyspark.sql.dataframe.DataFrame: \n",
    "    \n",
    "    \"\"\"\n",
    "    Ingest the latest Pubmed Abstract CSV file into a PySpark DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    filename = max(glob.iglob(f\"{path}/*.csv\"), key=os.path.getmtime)\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Year\", IntegerType(), True),\n",
    "        StructField(\"Country\", StringType(), True),\n",
    "        StructField(\"Journal\", StringType(), True),\n",
    "        StructField(\"DOI\", StringType(), True),\n",
    "        StructField(\"Abstract\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(filename)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2269e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame:\n",
    "    \n",
    "    # Transformation stages\n",
    "    tokenizer = Tokenizer(inputCol=\"Abstract\", outputCol=\"tokens\")\n",
    "    stopword_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered_tokens\")\n",
    "    lemmatizer = Lemmatizer(inputCol=stopword_remover.getOutputCol(), outputCol='lemmatized')\n",
    "     \n",
    "    # Feature stages: convert text into numerical features\n",
    "    count_vectorizer = CountVectorizer(inputCol=lemmatizer.getOutputCol(), outputCol=\"tf\")\n",
    "    idf = IDF(inputCol=count_vectorizer.getOutputCol(), outputCol=\"features\") # tf_idf\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        tokenizer,\n",
    "        stopword_remover,\n",
    "        lemmatizer,\n",
    "        count_vectorizer,\n",
    "        idf\n",
    "    ])\n",
    "    \n",
    "    # Fit the pipeline on the DataFrame\n",
    "    pipeline = pipeline.fit(df)\n",
    "    \n",
    "    # Apply the pipeline on the DataFrame\n",
    "    df_transformed = pipeline.transform(df)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_transformed = df_transformed.dropDuplicates()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_transformed = df_transformed.na.fill(\"Unknown\", subset=[\"Author\", \"Title\", \"Country\", \"Journal\", \"DOI\", \"Abstract\"])\n",
    "    df_transformed = df_transformed.na.fill(0, subset=[\"Year\"])\n",
    "    \n",
    "    # Standardize data formats\n",
    "    df_transformed = df_transformed.withColumn(\"Journal\", lower(col(\"Journal\")))\n",
    "    df_transformed = df_transformed.withColumn(\"DOI\", regexp_replace(col(\"DOI\"), \"^http(s)?://doi.org/\", \"\"))\n",
    "    \n",
    "    # Add incrementally increasing index\n",
    "    df_transformed = df_transformed.withColumn(\"index\", monotonically_increasing_id())\n",
    "    \n",
    "    return df_transformed, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82bce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path):\n",
    "    \"\"\"\n",
    "    Main function that puts everything together. \n",
    "    The function will expand as the PoC goes on.\n",
    "    \"\"\"\n",
    "    df = ingest_data(path)\n",
    "    transformed, pipeline = transform(df)\n",
    "    return transformed, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54fea1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----+-------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Author|               Title|Year|Country|             Journal|    DOI|            Abstract|              tokens|     filtered_tokens|          lemmatized|                  tf|            features|index|\n",
      "+------------+--------------------+----+-------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|  M J Carson|Simultaneous occu...|1977|Unknown|      am j dis child|Unknown|we report the cas...|[we, report, the,...|[report, cases, t...|[report, case, tw...|(21520,[0,1,2,4,7...|(21520,[0,1,2,4,7...|    0|\n",
      "| J Kavelaars|Hypernatremia in ...|2001|Unknown|          neth j med|Unknown|we describe a pat...|[we, describe, a,...|[describe, patien...|[describe, patien...|(21520,[1,2,4,7,1...|(21520,[1,2,4,7,1...|    1|\n",
      "|  A L London|Nephrogenic diabe...|1978|Unknown|          pediatrics|Unknown|nephrogenic diabe...|[nephrogenic, dia...|[nephrogenic, dia...|[nephrogenic, dia...|(21520,[1,2,4,45,...|(21520,[1,2,4,45,...|    2|\n",
      "|May Sanyoura|A novel ALMS1 spl...|2014| France|     eur j hum genet|Unknown|insulindependent ...|[insulindependent...|[insulindependent...|[insulindependent...|(21520,[0,1,2,4,5...|(21520,[0,1,2,4,5...|    3|\n",
      "|J Batcheller|Disorders of anti...|1992|Unknown|aacn clin issues ...|Unknown|depending upon th...|[depending, upon,...|[depending, upon,...|[depending, upon,...|(21520,[0,1,2,4,8...|(21520,[0,1,2,4,8...|    4|\n",
      "+------------+--------------------+----+-------+--------------------+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dir =\"../data/proc\"\n",
    "\n",
    "transformed, pipeline_model = main(data_dir)\n",
    "transformed.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07cf51",
   "metadata": {},
   "source": [
    "## The following code is work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128da00",
   "metadata": {},
   "source": [
    "Topic modelling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb00cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/02 21:47:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/06/02 21:47:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "dm\n",
      "corneal\n",
      "bb\n",
      "alanine\n",
      "limb\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "program\n",
      "spinal\n",
      "injury\n",
      "cord\n",
      "arrest\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "alpha\n",
      "pge\n",
      "adrenoceptor\n",
      "excretion\n",
      "\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "diabetic\n",
      "\n",
      "pregnancy\n",
      "control\n",
      "woman\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "wfs\n",
      "syndrome\n",
      "wolfram\n",
      "atrophy\n",
      "optic\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "bladder\n",
      "cat\n",
      "donor\n",
      "hz\n",
      "organ\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "camp\n",
      "tolbutamide\n",
      "phosphodiesterase\n",
      "hctz\n",
      "ilk\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "rat\n",
      "brattleboro\n",
      "di\n",
      "na\n",
      "nucleus\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "mirnas\n",
      "abscess\n",
      "nystagmus\n",
      "noteworthy\n",
      "bind\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "mri\n",
      "lymphocytic\n",
      "ma\n",
      "hypophysitis\n",
      "autoimmune\n",
      "\n",
      "\n",
      "Topic 10:\n",
      "neurosurgical\n",
      "cdi\n",
      "epidemiology\n",
      "pmolliter\n",
      "ttp\n",
      "\n",
      "\n",
      "Topic 11:\n",
      "\n",
      "diabetic\n",
      "glucose\n",
      "insulin\n",
      "p\n",
      "\n",
      "\n",
      "Topic 12:\n",
      "proximal\n",
      "microlitermin\n",
      "par\n",
      "cli\n",
      "delivery\n",
      "\n",
      "\n",
      "Topic 13:\n",
      "periodontal\n",
      "haematoma\n",
      "periodontitis\n",
      "dental\n",
      "microm\n",
      "\n",
      "\n",
      "Topic 14:\n",
      "vep\n",
      "bfv\n",
      "oa\n",
      "ophthalmic\n",
      "latency\n",
      "\n",
      "\n",
      "Topic 15:\n",
      "siadh\n",
      "mood\n",
      "li\n",
      "hyponatremia\n",
      "urate\n",
      "\n",
      "\n",
      "Topic 16:\n",
      "retinopathy\n",
      "\n",
      "heart\n",
      "diabetic\n",
      "ii\n",
      "\n",
      "\n",
      "Topic 17:\n",
      "pmolliter\n",
      "parenchyma\n",
      "net\n",
      "range\n",
      "glomerular\n",
      "\n",
      "\n",
      "Topic 18:\n",
      "crf\n",
      "anorectal\n",
      "ici\n",
      "tumor\n",
      "grading\n",
      "\n",
      "\n",
      "Topic 19:\n",
      "pmolliter\n",
      "nelson\n",
      "grl\n",
      "pituitary\n",
      "homeostatic\n",
      "\n",
      "\n",
      "Topic 20:\n",
      "platelet\n",
      "type\n",
      "ii\n",
      "altitude\n",
      "cofactor\n",
      "\n",
      "\n",
      "Topic 21:\n",
      "di\n",
      "pituitary\n",
      "cdi\n",
      "tumor\n",
      "idiopathic\n",
      "\n",
      "\n",
      "Topic 22:\n",
      "viscosity\n",
      "breast\n",
      "aquaporins\n",
      "aggregation\n",
      "redcell\n",
      "\n",
      "\n",
      "Topic 23:\n",
      "finger\n",
      "professional\n",
      "healthcare\n",
      "anesthesia\n",
      "constriction\n",
      "\n",
      "\n",
      "Topic 24:\n",
      "fndi\n",
      "neuron\n",
      "ggt\n",
      "gammaglutamyl\n",
      "aggregate\n",
      "\n",
      "\n",
      "Topic 25:\n",
      "revised\n",
      "proof\n",
      "differents\n",
      "roll\n",
      "sistematized\n",
      "\n",
      "\n",
      "Topic 26:\n",
      "bloodglucose\n",
      "hba\n",
      "storage\n",
      "pancreatic\n",
      "oxidation\n",
      "\n",
      "\n",
      "Topic 27:\n",
      "avp\n",
      "\n",
      "rat\n",
      "vasopressin\n",
      "water\n",
      "\n",
      "\n",
      "Topic 28:\n",
      "drw\n",
      "al\n",
      "bf\n",
      "linkage\n",
      "et\n",
      "\n",
      "\n",
      "Topic 29:\n",
      "l\n",
      "indapamide\n",
      "indicum\n",
      "solanum\n",
      "man\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Split the transformed data into training and test sets\n",
    "train_data, test_data = transformed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# LDA model\n",
    "lda = LDA(k=30, maxIter=10, featuresCol=\"features\")  # Specify the number of topics (k) and iterations\n",
    "lda_model = lda.fit(train_data)\n",
    "\n",
    "wordNumbers = 5\n",
    "\n",
    "def topic_render(topic, vocabulary):\n",
    "    terms = topic[1]\n",
    "    result = []\n",
    "    for i in range(wordNumbers):\n",
    "        term = vocabulary[terms[i]]\n",
    "        result.append(term)\n",
    "    return result\n",
    "\n",
    "count_vectorizer_model = pipeline_model.stages[3]\n",
    "vocabulary = count_vectorizer_model.vocabulary\n",
    "topics_final = lda_model.describeTopics(maxTermsPerTopic=wordNumbers).rdd.map(lambda x: topic_render(x, vocabulary)).collect()\n",
    "\n",
    "for topic in range(len(topics_final)):\n",
    "    print(f\"Topic {topic}:\")\n",
    "    for term in topics_final[topic]:\n",
    "        print(term)\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115e4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 12.691801656873688\n"
     ]
    }
   ],
   "source": [
    "# Evaluate perplexity on the test data\n",
    "perplexity = lda_model.logPerplexity(test_data)\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6737b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create abbreviation/acronym replacer\n",
    "# TODO: Hyperparameter tuning of basemodel\n",
    "# TODO: Model evaluation\n",
    "# TODO: Visualize topics (with Bokeh or Dash?)\n",
    "# TODO: Visualize publications per country per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ebf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-spark test\n",
    "import spacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "# Add the abbreviation pipe to the spacy pipeline.\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "doc = nlp(\n",
    "    \"Chronic lymphocytic leukemia (CLL), autoimmune hemolytic anemia, and oral ulcer. The patient was diagnosed with chronic lymphocytic leukemia and was noted to have autoimmune hemolytic anemia at the time of his CLL diagnosis.\"\n",
    ")\n",
    "fmt_str = \"{:<6}| {:<30}| {:<6}| {:<6}\"\n",
    "print(fmt_str.format(\"Short\", \"Long\", \"Starts\", \"Ends\"))\n",
    "for abrv in doc._.abbreviations:\n",
    "    print(fmt_str.format(abrv.text, str(abrv._.long_form), abrv.start, abrv.end))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: Abbreviation replacement using scispacy\n",
    "import spacy\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "def convert_abbreviations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert abbreviations to text using SciSpacy.\n",
    "    :param text: Input text data.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_sci_md\")\n",
    "    abbreviation_pipe = nlp.create_pipe(\"abbreviation_detector\")\n",
    "    nlp.add_pipe(\"abbreviation_detector\")\n",
    "    doc = nlp(text)\n",
    "    altered_tok = [tok.text for tok in doc]\n",
    "    for abrv in doc._.abbreviations:\n",
    "        altered_tok[abrv.start] = str(abrv._.long_form)\n",
    "    return \" \".join(altered_tok)\n",
    "\n",
    "\n",
    "# Create a UDF from the convert_abbreviations function\n",
    "convert_abbreviations_udf = udf(convert_abbreviations, StringType())\n",
    "\n",
    "# Apply the UDF to the text_column\n",
    "test = transformed.withColumn(\"converted_text\", convert_abbreviations_udf(transformed[\"Abstract\"]))\n",
    "\n",
    "test.select(test[\"converted_text\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ae9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_spark",
   "language": "python",
   "name": "venv_spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
